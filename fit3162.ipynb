{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.QVLO2T66WEPI7JZ63PS3HMOHFEY472BC.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "# Variables that contains the credentials to access Twitter API\n",
    "ACCESS_TOKEN = '1472638099490439170-yIq47dBq8EcUuO44msP5CuggeeZBml'\n",
    "ACCESS_SECRET = 'swIQSmcUdqZ8XYlzrvnF08IBuzamva5W6b8dUDuXAFxkT'\n",
    "CONSUMER_KEY = '5etHstBqMcYpIDESsQPvvhyJO'\n",
    "CONSUMER_SECRET = 'cgsmFFYBHPZX7QqeauuwQQgdLgM2RvLnMbvDyJoHYCuOEKo0Gd'\n",
    "\n",
    "\n",
    "# Setup access to API\n",
    "def connect_to_twitter_OAuth():\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    "\n",
    "\n",
    "# Create API object\n",
    "api = connect_to_twitter_OAuth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"digging your own grave\" https://t.co/ykc7XMz5Ky\n",
      "just adopted a cat and he is in love with rowlet! https://t.co/yLl7tTwxmI\n",
      "can't believe i had been spamming @gingerrootmusic's city slicker so much https://t.co/Xs2jvTQ9bB\n",
      "been feeding her for 3 days, she is starting to accept eating in close proximity to me (maybe 20cm away)\n",
      "does anyone know how to build trust with a feral cat? https://t.co/3nlkpy3U0f\n",
      "manifesting to get a black cat https://t.co/LnnBvTqN7Z\n",
      "RT @Minecraft: In third place it‚Äôs the Glare, we guess you aren‚Äôt as afraid of the dark as we are? That leaves the Allay and the Copper Gol‚Ä¶\n",
      "seeing someone who allegedly sexually assaulted others posting about \"raising awareness for women who were sexually‚Ä¶ https://t.co/bu3qqBUZLp\n",
      "in solidarity with those who participated in the protest today\n",
      "we will be back soon, only with the more of us and s‚Ä¶ https://t.co/9TvR7OaWLU\n",
      "democracy is dead again, it's has gone way too far this time #Lawan\n",
      "RT @karmunloh_: Today itself:\n",
      "\n",
      "- 4 more youth activists were called in by the police.\n",
      "- Istana Negara revealed #KerajaanDerhaka did not rec‚Ä¶\n",
      "RT @WeegeeWuvver: thinking about that backpack rowlet plush with unfortunate packaging https://t.co/22m1a3hal9\n",
      "RT @pokejungle: The Pok√©mon Grassy Gardening collection has been revealed for release in Japan! Features ADORABLE potted plushies and hangi‚Ä¶\n",
      "RT @KagiMusic: OTT Resource Pack for Minecraft\n",
      "\n",
      "Real (not fake) music producers use this sound pack. https://t.co/OeriaNZnpE\n",
      "You ain't scared to have Jesus on your timeline right? \n",
      "@thtstunalicious\n",
      "as i just thought that i am actually getting better, but turns out my mental health has spiralled into madness once again unknowingly LOL\n",
      "RT @mokumokumonsu: „Çπ„Çø„Éº„É¢„ÉÉ„ÇØ„Çπ https://t.co/BPCwlcU411\n",
      "RT @ymrn891: „Çå„Åò„Åá„Çì„Å©ÔºÅ https://t.co/yZU8S9N6h9\n",
      "just realised i had reposted my profile picture LOL\n",
      "#„É¢„ÇØ„É≠„Éº„Å´„Åç„ÇÅ„Åü \n",
      "‚ú®‚ú® https://t.co/yM5L1vr6cK\n"
     ]
    }
   ],
   "source": [
    "# tweets from my stream\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Script to Extract tweets of a\n",
    "# particular Hashtag using Tweepy and Pandas\n",
    " \n",
    "# function to display data of each tweet\n",
    "def printtweetdata(n, ith_tweet):\n",
    "        print()\n",
    "        print(f\"Tweet {n}:\")\n",
    "        print(f\"Username:{ith_tweet[0]}\")\n",
    "        print(f\"Description:{ith_tweet[1]}\")\n",
    "        print(f\"Location:{ith_tweet[2]}\")\n",
    "        print(f\"Following Count:{ith_tweet[3]}\")\n",
    "        print(f\"Follower Count:{ith_tweet[4]}\")\n",
    "        print(f\"Total Tweets:{ith_tweet[5]}\")\n",
    "        print(f\"Retweet Count:{ith_tweet[6]}\")\n",
    "        print(f\"Tweet Text:{ith_tweet[7]}\")\n",
    "        print(f\"Hashtags Used:{ith_tweet[8]}\")\n",
    " \n",
    " \n",
    "# function to perform data extraction\n",
    "def scrape(words, date_since, numtweet, filename):\n",
    " \n",
    "        # Creating DataFrame using pandas\n",
    "        db = pd.DataFrame(columns=['username',\n",
    "                                   'description',\n",
    "                                   'location',\n",
    "                                   'following',\n",
    "                                   'followers',\n",
    "                                   'totaltweets',\n",
    "                                   'retweetcount',\n",
    "                                   'text',\n",
    "                                   'hashtags'])\n",
    " \n",
    "        # We are using .Cursor() to search\n",
    "        # through twitter for the required tweets.\n",
    "        # The number of tweets can be\n",
    "        # restricted using .items(number of tweets)\n",
    "        tweets = tweepy.Cursor(api.search_tweets,\n",
    "                               words, lang=\"en\",\n",
    "                               since_id=date_since,\n",
    "                               tweet_mode='extended').items(numtweet)\n",
    " \n",
    " \n",
    "        # .Cursor() returns an iterable object. Each item in\n",
    "        # the iterator has various attributes\n",
    "        # that you can access to\n",
    "        # get information about each tweet\n",
    "        list_tweets = [tweet for tweet in tweets]\n",
    " \n",
    "        # Counter to maintain Tweet Count\n",
    "        i = 1\n",
    " \n",
    "        # we will iterate over each tweet in the\n",
    "        # list for extracting information about each tweet\n",
    "        for tweet in list_tweets:\n",
    "                username = tweet.user.screen_name\n",
    "                description = tweet.user.description\n",
    "                location = tweet.user.location\n",
    "                following = tweet.user.friends_count\n",
    "                followers = tweet.user.followers_count\n",
    "                totaltweets = tweet.user.statuses_count\n",
    "                retweetcount = tweet.retweet_count\n",
    "                hashtags = tweet.entities['hashtags']\n",
    " \n",
    "                # Retweets can be distinguished by\n",
    "                # a retweeted_status attribute,\n",
    "                # in case it is an invalid reference,\n",
    "                # except block will be executed\n",
    "                try:\n",
    "                        text = tweet.retweeted_status.full_text\n",
    "                except AttributeError:\n",
    "                        text = tweet.full_text\n",
    "                hashtext = list()\n",
    "                for j in range(0, len(hashtags)):\n",
    "                        hashtext.append(hashtags[j]['text'])\n",
    " \n",
    "                # Here we are appending all the\n",
    "                # extracted information in the DataFrame\n",
    "                ith_tweet = [username, description,\n",
    "                             location, following,\n",
    "                             followers, totaltweets,\n",
    "                             retweetcount, text, hashtext]\n",
    "                db.loc[len(db)] = ith_tweet\n",
    " \n",
    "                # Function call to print tweet data on screen\n",
    "                printtweetdata(i, ith_tweet)\n",
    "                i = i+1\n",
    " \n",
    "                # we will save our database as a CSV file.\n",
    "                db.to_csv(filename)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWarning: do not execute this unless scraping new data\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Warning: do not execute this unless scraping new data\n",
    "\"\"\"\n",
    "# # Hashtag and initial date for query\n",
    "# words = 'cryptocurrency'\n",
    "# date_since = '2022-01--10'\n",
    "# # output fileanme\n",
    "# filename = 'scraped_hashtag_tweets_' + date_since + '_02.csv'\n",
    "\n",
    "# # number of tweets you want to extract in one run\n",
    "# numtweet = 900\n",
    "# scrape(words, date_since, numtweet, filename)\n",
    "# print('Scraping has completed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>username</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OmarFar47417199</td>\n",
       "      <td>Koli ID: plRCiGz7X56Y052mQwGeuZGXEdvydLohIDe25...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3815</td>\n",
       "      <td>21</td>\n",
       "      <td>5381</td>\n",
       "      <td>0</td>\n",
       "      <td>@CryptoTownEU wow That‚Äôs great very strong pro...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>JoeyStocksNBnds</td>\n",
       "      <td>Altcoins and NFTs all day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>183</td>\n",
       "      <td>7790</td>\n",
       "      <td>1</td>\n",
       "      <td>@ShillMe I did! This coins a no brainer buy. W...</td>\n",
       "      <td>['kiba', 'cryptocurrency', 'memecoin']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Day_Quintero</td>\n",
       "      <td>Moon of his life üåô‚ú®\\n#BRGArmy</td>\n",
       "      <td>Santa Marta, Colombia</td>\n",
       "      <td>1325</td>\n",
       "      <td>75</td>\n",
       "      <td>4001</td>\n",
       "      <td>1147</td>\n",
       "      <td>üí•Here is our SUPERIOR LAND in Martian Home!üòç\\n...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>crypairdrop22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135</td>\n",
       "      <td>6</td>\n",
       "      <td>428</td>\n",
       "      <td>85</td>\n",
       "      <td>Drop your ETH Address. \\n#ETH #Ethereum #crypt...</td>\n",
       "      <td>['ETH', 'Ethereum', 'cryptocurrency']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>timmysnotes</td>\n",
       "      <td>Global level IT for couple of decades w. IBM, ...</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>2666</td>\n",
       "      <td>541</td>\n",
       "      <td>26933</td>\n",
       "      <td>2</td>\n",
       "      <td>#uninstallnorton\\n\\n#Norton put a cryptominer ...</td>\n",
       "      <td>['uninstallnorton', 'Norton', 'antivirus']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         username  \\\n",
       "0           0  OmarFar47417199   \n",
       "1           1  JoeyStocksNBnds   \n",
       "2           2     Day_Quintero   \n",
       "3           3    crypairdrop22   \n",
       "4           4      timmysnotes   \n",
       "\n",
       "                                         description               location  \\\n",
       "0  Koli ID: plRCiGz7X56Y052mQwGeuZGXEdvydLohIDe25...                    NaN   \n",
       "1                          Altcoins and NFTs all day                    NaN   \n",
       "2                      Moon of his life üåô‚ú®\\n#BRGArmy  Santa Marta, Colombia   \n",
       "3                                                NaN                    NaN   \n",
       "4  Global level IT for couple of decades w. IBM, ...                Ireland   \n",
       "\n",
       "   following  followers  totaltweets  retweetcount  \\\n",
       "0       3815         21         5381             0   \n",
       "1        200        183         7790             1   \n",
       "2       1325         75         4001          1147   \n",
       "3        135          6          428            85   \n",
       "4       2666        541        26933             2   \n",
       "\n",
       "                                                text  \\\n",
       "0  @CryptoTownEU wow That‚Äôs great very strong pro...   \n",
       "1  @ShillMe I did! This coins a no brainer buy. W...   \n",
       "2  üí•Here is our SUPERIOR LAND in Martian Home!üòç\\n...   \n",
       "3  Drop your ETH Address. \\n#ETH #Ethereum #crypt...   \n",
       "4  #uninstallnorton\\n\\n#Norton put a cryptominer ...   \n",
       "\n",
       "                                     hashtags  \n",
       "0                                          []  \n",
       "1      ['kiba', 'cryptocurrency', 'memecoin']  \n",
       "2                                          []  \n",
       "3       ['ETH', 'Ethereum', 'cryptocurrency']  \n",
       "4  ['uninstallnorton', 'Norton', 'antivirus']  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag = pd.read_csv(r\"C:\\Users\\User\\Documents\\C2001\\FIT3161_3162\\FIT3161\\scraped_hashtag_tweets_2022-01--09.csv\") \n",
    "# df_eth.columns = df_eth.iloc[0]\n",
    "# df_eth.drop(df_eth.index[[0, 1]], inplace=True)\n",
    "hashtag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the unnamed column\n",
    "hashtag.drop(hashtag.columns[0], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OmarFar47417199</td>\n",
       "      <td>Koli ID: plRCiGz7X56Y052mQwGeuZGXEdvydLohIDe25...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3815</td>\n",
       "      <td>21</td>\n",
       "      <td>5381</td>\n",
       "      <td>0</td>\n",
       "      <td>@CryptoTownEU wow That‚Äôs great very strong pro...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JoeyStocksNBnds</td>\n",
       "      <td>Altcoins and NFTs all day</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>183</td>\n",
       "      <td>7790</td>\n",
       "      <td>1</td>\n",
       "      <td>@ShillMe I did! This coins a no brainer buy. W...</td>\n",
       "      <td>['kiba', 'cryptocurrency', 'memecoin']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Day_Quintero</td>\n",
       "      <td>Moon of his life üåô‚ú®\\n#BRGArmy</td>\n",
       "      <td>Santa Marta, Colombia</td>\n",
       "      <td>1325</td>\n",
       "      <td>75</td>\n",
       "      <td>4001</td>\n",
       "      <td>1147</td>\n",
       "      <td>üí•Here is our SUPERIOR LAND in Martian Home!üòç\\n...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crypairdrop22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135</td>\n",
       "      <td>6</td>\n",
       "      <td>428</td>\n",
       "      <td>85</td>\n",
       "      <td>Drop your ETH Address. \\n#ETH #Ethereum #crypt...</td>\n",
       "      <td>['ETH', 'Ethereum', 'cryptocurrency']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>timmysnotes</td>\n",
       "      <td>Global level IT for couple of decades w. IBM, ...</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>2666</td>\n",
       "      <td>541</td>\n",
       "      <td>26933</td>\n",
       "      <td>2</td>\n",
       "      <td>#uninstallnorton\\n\\n#Norton put a cryptominer ...</td>\n",
       "      <td>['uninstallnorton', 'Norton', 'antivirus']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username                                        description  \\\n",
       "0  OmarFar47417199  Koli ID: plRCiGz7X56Y052mQwGeuZGXEdvydLohIDe25...   \n",
       "1  JoeyStocksNBnds                          Altcoins and NFTs all day   \n",
       "2     Day_Quintero                      Moon of his life üåô‚ú®\\n#BRGArmy   \n",
       "3    crypairdrop22                                                NaN   \n",
       "4      timmysnotes  Global level IT for couple of decades w. IBM, ...   \n",
       "\n",
       "                location  following  followers  totaltweets  retweetcount  \\\n",
       "0                    NaN       3815         21         5381             0   \n",
       "1                    NaN        200        183         7790             1   \n",
       "2  Santa Marta, Colombia       1325         75         4001          1147   \n",
       "3                    NaN        135          6          428            85   \n",
       "4                Ireland       2666        541        26933             2   \n",
       "\n",
       "                                                text  \\\n",
       "0  @CryptoTownEU wow That‚Äôs great very strong pro...   \n",
       "1  @ShillMe I did! This coins a no brainer buy. W...   \n",
       "2  üí•Here is our SUPERIOR LAND in Martian Home!üòç\\n...   \n",
       "3  Drop your ETH Address. \\n#ETH #Ethereum #crypt...   \n",
       "4  #uninstallnorton\\n\\n#Norton put a cryptominer ...   \n",
       "\n",
       "                                     hashtags  \n",
       "0                                          []  \n",
       "1      ['kiba', 'cryptocurrency', 'memecoin']  \n",
       "2                                          []  \n",
       "3       ['ETH', 'Ethereum', 'cryptocurrency']  \n",
       "4  ['uninstallnorton', 'Norton', 'antivirus']  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping NaN line\n",
    "hashtag = hashtag.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing newline and tab\n",
    "hashtag = hashtag.replace(r'\\n',' ', regex=True) \n",
    "hashtag = hashtag.replace(r'\\t',' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Day_Quintero</td>\n",
       "      <td>Moon of his life üåô‚ú® #BRGArmy</td>\n",
       "      <td>Santa Marta, Colombia</td>\n",
       "      <td>1325</td>\n",
       "      <td>75</td>\n",
       "      <td>4001</td>\n",
       "      <td>1147</td>\n",
       "      <td>üí•Here is our SUPERIOR LAND in Martian Home!üòç  ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>timmysnotes</td>\n",
       "      <td>Global level IT for couple of decades w. IBM, ...</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>2666</td>\n",
       "      <td>541</td>\n",
       "      <td>26933</td>\n",
       "      <td>2</td>\n",
       "      <td>#uninstallnorton  #Norton put a cryptominer in...</td>\n",
       "      <td>['uninstallnorton', 'Norton', 'antivirus']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NDGZARMY</td>\n",
       "      <td>New 1000x token @nasadogezilla</td>\n",
       "      <td>Usagre, Espa√±a</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>687</td>\n",
       "      <td>0</td>\n",
       "      <td>@CryptosFun @umituys20161083 if u missed #baby...</td>\n",
       "      <td>['babydoge', 'nasadoge', 'dogezilla', 'BSCGems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mustafa37312657</td>\n",
       "      <td>√áok g√ºzelsin airdrop</td>\n",
       "      <td>ƒ∞stanbul</td>\n",
       "      <td>307</td>\n",
       "      <td>3</td>\n",
       "      <td>558</td>\n",
       "      <td>385</td>\n",
       "      <td>üöÄ Airdrop: KINGDOM DEFENSE üí∞ Value: $10,000,00...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thinkankit</td>\n",
       "      <td>International Property Investment|More then 9 ...</td>\n",
       "      <td>UAE</td>\n",
       "      <td>83</td>\n",
       "      <td>394</td>\n",
       "      <td>294</td>\n",
       "      <td>5</td>\n",
       "      <td>Bitcoin falls by 8% as #Kazakhstan internet sh...</td>\n",
       "      <td>['Kazakhstan']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username                                        description  \\\n",
       "2     Day_Quintero                       Moon of his life üåô‚ú® #BRGArmy   \n",
       "4      timmysnotes  Global level IT for couple of decades w. IBM, ...   \n",
       "5         NDGZARMY                     New 1000x token @nasadogezilla   \n",
       "6  Mustafa37312657                               √áok g√ºzelsin airdrop   \n",
       "9       thinkankit  International Property Investment|More then 9 ...   \n",
       "\n",
       "                location  following  followers  totaltweets  retweetcount  \\\n",
       "2  Santa Marta, Colombia       1325         75         4001          1147   \n",
       "4                Ireland       2666        541        26933             2   \n",
       "5         Usagre, Espa√±a          7          7          687             0   \n",
       "6               ƒ∞stanbul        307          3          558           385   \n",
       "9                    UAE         83        394          294             5   \n",
       "\n",
       "                                                text  \\\n",
       "2  üí•Here is our SUPERIOR LAND in Martian Home!üòç  ...   \n",
       "4  #uninstallnorton  #Norton put a cryptominer in...   \n",
       "5  @CryptosFun @umituys20161083 if u missed #baby...   \n",
       "6  üöÄ Airdrop: KINGDOM DEFENSE üí∞ Value: $10,000,00...   \n",
       "9  Bitcoin falls by 8% as #Kazakhstan internet sh...   \n",
       "\n",
       "                                            hashtags  \n",
       "2                                                 []  \n",
       "4         ['uninstallnorton', 'Norton', 'antivirus']  \n",
       "5  ['babydoge', 'nasadoge', 'dogezilla', 'BSCGems...  \n",
       "6                                                 []  \n",
       "9                                     ['Kazakhstan']  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all the tweets into one list for pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tweets = np.array(hashtag['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = []\n",
    "\n",
    "for text in tweets:\n",
    "    clean_tweets.append(text.split(' '))\n",
    "\n",
    "#print(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining all the tweets into a sentence\n",
    "join_tweets_sent = [j for i in clean_tweets for j in i]\n",
    "# print(join_tweets_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove space from list\n",
    "join_tweets_sent.remove('')\n",
    "#print(' Updated List: ', join_tweets_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining word in list\n",
    "# tweets_sent = ' '.join(join_tweets_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing special characters and combining word in list\n",
    "tweets_sent = ' '.join(e for e in join_tweets_sent if e.isalnum())\n",
    "#print(tweets_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = nltk.word_tokenize(tweets_sent)\n",
    "tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "tokens_l = [w.lower() for w in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 1166 samples and 3468 outcomes>\n"
     ]
    }
   ],
   "source": [
    "#  Using a Tagger. Which is part-of-speech \n",
    "tagged = nltk.pos_tag(tokens_l)\n",
    "#print(tagged)\n",
    "\n",
    "\n",
    "# extract only Noun\n",
    "only_nn = [x for (x,y) in tagged if y in ('NN') or ('JJ')]\n",
    "\n",
    "freq = nltk.FreqDist(only_nn)\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 109), ('cryptocurrency', 68), ('the', 65), ('chance', 50), ('i', 41), ('u', 38), ('worry', 36), ('fair', 36), ('dev', 36), ('details', 36)]\n"
     ]
    }
   ],
   "source": [
    "print(freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the top mentions\n",
    "(should be plot trends over time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "term = [x[0] for x in freq.most_common(10)]\n",
    "frequency = [x[1] for x in freq.most_common(10)]\n",
    "\n",
    "plt.style.use('seaborn-pastel')\n",
    "\n",
    "plt.barh(term,frequency)\n",
    "plt.rc('axes', titlesize=20)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=20)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=13)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=13)    # fontsize of the tick labels\n",
    "plt.title('Top 10 most mentioned term')\n",
    "plt.ylabel('Term')\n",
    "plt.xlabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.5.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>√à appena uscito un nuovo video! LES CRYPTOMONN...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Cardano: Digitize Currencies; EOS https://t.co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Another Test tweet that wasn't caught in the s...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Current Crypto Prices! \\n\\nBTC: $8721.99 USD\\n...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Spiv (Nosar Baz): BITCOIN Is An Asset &amp;amp; NO...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               text Sentiment\n",
       "0  2019-05-27  √à appena uscito un nuovo video! LES CRYPTOMONN...  Positive\n",
       "1  2019-05-27  Cardano: Digitize Currencies; EOS https://t.co...  Positive\n",
       "2  2019-05-27  Another Test tweet that wasn't caught in the s...  Positive\n",
       "3  2019-05-27  Current Crypto Prices! \\n\\nBTC: $8721.99 USD\\n...  Positive\n",
       "4  2019-05-27  Spiv (Nosar Baz): BITCOIN Is An Asset &amp; NO...  Positive"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load bitcoin sentiment dataset\n",
    "df_bit = pd.read_csv(\"C:/Users/User/Documents/C2001/FIT3161_3162/FIT3161/dataset/sentiment/mbsa.csv\")\n",
    "df_bit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date              0\n",
       "text             16\n",
       "Sentiment    891144\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is null \n",
    "df_bit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date         0\n",
       "text         0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping null rows\n",
    "df_bit = df_bit.dropna()\n",
    "df_bit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove new line and tab\n",
    "df_bit = df_bit.replace(r'\\n',' ', regex=True) \n",
    "df_bit = df_bit.replace(r'\\t',' ', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>√à appena uscito un nuovo video  LES CRYPTOMONN...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Cardano  Digitize Currencies  EOS https   t co...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Another Test tweet that wasn t caught in the s...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Current Crypto Prices    BTC   8721 99 USD ETH...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-27</td>\n",
       "      <td>Spiv  Nosar Baz   BITCOIN Is An Asset  amp  NO...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                               text Sentiment\n",
       "0  2019-05-27  √à appena uscito un nuovo video  LES CRYPTOMONN...  Positive\n",
       "1  2019-05-27  Cardano  Digitize Currencies  EOS https   t co...  Positive\n",
       "2  2019-05-27  Another Test tweet that wasn t caught in the s...  Positive\n",
       "3  2019-05-27  Current Crypto Prices    BTC   8721 99 USD ETH...  Positive\n",
       "4  2019-05-27  Spiv  Nosar Baz   BITCOIN Is An Asset  amp  NO...  Positive"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing special characters\n",
    "df_bit['text'] = df_bit['text'].str.replace('\\W', ' ')\n",
    "df_bit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing string label with integer\n",
    "df_bit['Sentiment'] = df_bit['Sentiment'].replace(\"Positive\", 1, regex=True)\n",
    "df_bit['Sentiment'] = df_bit['Sentiment'].replace(\"Negative\", 0, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the date column\n",
    "df_bit = df_bit.drop(columns=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIG Blockchain Intelligence Group announces acquisition of Netcoins Operations  entry into Bitcoin Institutional OTC Trading and Cryptocurrency Custody Services   https   t co 64CtBRv9uV 1\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = df_bit['text'], df_bit['Sentiment']\n",
    "\n",
    "# testing\n",
    "index = 101\n",
    "print(X_train[index], Y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max length of the X_train data\n",
    "maxLen = len(max(X_train, key=len).split()) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading glove vectors dataset\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('C:/Users/User/Documents/GNey/glove.6B/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13775686 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each tweets.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df_bit['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 17.2 GiB for an array with shape (18452904, 250) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-49cdfc98ab9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_bit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Shape of data tensor:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    152\u001b[0m   return sequence.pad_sequences(\n\u001b[0;32m    153\u001b[0m       \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m keras_export(\n",
      "\u001b[1;32mC:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     83\u001b[0m                          .format(dtype, type(value)))\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 17.2 GiB for an array with shape (18452904, 250) and data type int32"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df_bit['text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(df_bit['Sentiment']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer function\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  \n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]     \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word] \n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model\n",
    "def lstm_bitcoin(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = tf.keras.Input(input_shape, dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    X = tf.keras.layers.LSTM(64, return_sequences=True, activation = 'tanh')(embeddings)\n",
    "    X = tf.keras.layers.Dropout(0.4)(X)\n",
    "    X = tf.keras.layers.LSTM(64, return_sequences=False, activation = 'tanh')(X)\n",
    "    X = tf.keras.layers.Dropout(0.4)(X)\n",
    "    X = tf.keras.layers.Dense(len(X_train))(X)\n",
    "    X = tf.keras.layers.Activation('softmax')(X)\n",
    "    model = tf.keras.Model(inputs=sentence_indices, outputs=X)   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 58)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 58, 100)           40000100  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 58, 64)            42240     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 58, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 18452904)          1199438760\n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 18452904)          0         \n",
      "=================================================================\n",
      "Total params: 1,239,514,124\n",
      "Trainable params: 1,199,514,024\n",
      "Non-trainable params: 40,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = lstm_bitcoin((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 41.2 GiB for an array with shape (18452904, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-7b9a0fa94b68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX_train_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences_to_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxLen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mY_train_one_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_one_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-eff68595e973>\u001b[0m in \u001b[0;36msentences_to_indices\u001b[1;34m(X, word_to_index, max_len)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msentences_to_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0msentence_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 41.2 GiB for an array with shape (18452904, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "# converting the indices into one hot vector\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_one_hot = convert_to_one_hot(Y_train, C = len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X_train_indices, Y_train, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_test = np.array([['Email'],['images'],['picture'],['pictures'],['Open'],['Location'],['Date'],['Year'],['Address'],['Close']])\n",
    "#print(maxLen)\n",
    "\n",
    "for x in x_test:\n",
    "    X_test_indices = sentences_to_indices(x, word_to_index, maxLen)\n",
    "    #print(X_test_indices)\n",
    "    print(np.argmax(model.predict(X_test_indices)))\n",
    "    print(x[0] +'  '+ X_train1[np.argmax(model.predict(X_test_indices))])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
